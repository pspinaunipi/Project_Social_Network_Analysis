{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import numpy as np\n",
    "import linkpred\n",
    "from linkpred.evaluation import Pair\n",
    "\n",
    "\n",
    "def find_common(g,k):\n",
    "    \"\"\"\n",
    "    This function deletes all the nodes in g which are adiacent to less than k edges\n",
    "    \"\"\"\n",
    "    nodes_to_remove = []\n",
    "    start = g.number_of_nodes()\n",
    "    # for each node delete all the nodes wich have less than 3 adiacent edges\n",
    "    for node in g.nodes():\n",
    "        if len(g.in_edges(node)) + len(g.out_edges(node))  < k:\n",
    "            nodes_to_remove.append(node)\n",
    "    for node in nodes_to_remove:\n",
    "        g.remove_node(node)\n",
    "    # print the number of nodes we deleted\n",
    "    finish = g.number_of_nodes()\n",
    "    print(\"{} nodes removed\".format(start-finish))\n",
    "    return g\n",
    "\n",
    "def remove_useless_data(df):\n",
    "    \"\"\"\n",
    "    Simple function used to remove from a dataframe all the rows in which the subreddit \"parent\"\n",
    "    is equal to the subreddit \"to\"\n",
    "    \"\"\"\n",
    "    df_1 = df[df[\"parent\"] == df[\"to\"]]\n",
    "    # delete duplicate rows from dataframe\n",
    "    new_df = df.drop(df_1.index)\n",
    "    #delete useless data to free memory\n",
    "    del df,df_1\n",
    "    gc.collect()\n",
    "    #reset the indexes in the new DataFrame\n",
    "    new_df.reset_index(inplace=True,drop=True)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def remove_uncommon(g_1,g_2):\n",
    "    \"\"\"\n",
    "    This function removes from a graph g_1 all the nodes that are not present in\n",
    "    another graph g_2\n",
    "    \"\"\"\n",
    "    nodes_to_remove = []\n",
    "    for node in g_1.nodes():\n",
    "        if node not in g_2.nodes():\n",
    "            nodes_to_remove.append(node)\n",
    "    for node in nodes_to_remove:\n",
    "        g_1.remove_node(node)\n",
    "    return g_1\n",
    "\n",
    "def find_core(g_tr,g_te,k_train,k_test):\n",
    "    \"\"\"\n",
    "    This function finds the core graph defined in the article ...\n",
    "\n",
    "    \"\"\"\n",
    "    # find all the nodes adiacent at least to k edges\n",
    "    g_tr = find_common(g_train,k_train)\n",
    "    g_te = find_common(g_test,k_test)\n",
    "    # remove all the nodes that are not present in both train and test graph\n",
    "    g_tr_copy = g_tr.copy()\n",
    "    g_te_copy = g_te.copy()\n",
    "    g_tr1 = remove_uncommon(g_tr,g_te_copy)\n",
    "    g_te1 = remove_uncommon(g_te,g_tr_copy)\n",
    "    # merge the two graphs to obtain the core graph\n",
    "    g_core = nx.compose(g_tr1,g_te1)\n",
    "    return g_core\n",
    "\n",
    "def accuracy_pred(g,results):\n",
    "    \"\"\"\n",
    "    Simple function to compute the accuracy of a classifier\n",
    "    \"\"\"\n",
    "    right_pred = 0\n",
    "\n",
    "    for result in results:\n",
    "        if (result[0],result[1]) in g.edges():\n",
    "            right_pred = right_pred+1\n",
    "        elif (result[1],result[0]) in g.edges():\n",
    "            right_pred = right_pred+1\n",
    "    accuracy = right_pred/(len(results))\n",
    "    return accuracy\n",
    "\n",
    "def predict_link(g,pred,predictions,name,**kwargs):\n",
    "    \"\"\"\n",
    "    This function starts to predict return accuracy\n",
    "    \"\"\"\n",
    "    pred_results = pred.predict(**kwargs)\n",
    "    top_pred = pred_results.top(predictions)\n",
    "    acc = accuracy_pred(g,top_pred)\n",
    "    print(\"Accuracy {} predictor: {:.2f}%\".format(name,acc*100))\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the graph into train and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The first thing I did was to split the dataset into two parts:<br>\n",
    "1 Training set <br>\n",
    "2 Test set <br>\n",
    "    \n",
    "I decided to use as split point the 12th of april 2021. \n",
    "This way the training-test split correspond roughly to the 80%-20% of the total data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique posts 120747\n",
      "Edges in trainig set 159926\n",
      "Edges in test set 39279\n"
     ]
    }
   ],
   "source": [
    "#set the date that separates test and training set\n",
    "START_DATE = 1618272000\n",
    "# load and clean data\n",
    "data = pd.read_csv(\"../scraping data/data/data_subreddit_cleaned.csv\",index_col=0)\n",
    "data = remove_useless_data(data)\n",
    "#divide in training and test set\n",
    "data_train = data[data[\"date\"] < START_DATE]\n",
    "data_test = data[data[\"date\"] >= START_DATE]\n",
    "print(\"Number of unique posts {}\".format(len(pd.unique(data[\"title\"]))))\n",
    "print(\"Edges in trainig set {}\\nEdges in test set {}\".format(data_train.shape[0],data_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Dividing the dataset into two parts is not enought.<br> \n",
    "What i did was to delete all the nodes from the training and test set that are not adiacent to at least 3 nodes. <br>\n",
    "This way I eliminated all the subreddit that are not likely to interact with each other.<br>\n",
    "Lastly i created a new graph that contains nodes that are present both in the training and test set.<br>\n",
    "This graph is called core and rapresents the most active subreddits.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12721 nodes removed\n",
      "6232 nodes removed\n",
      "Nodes in core 3364\n",
      "Edges in core 107167\n",
      "Edges in E_old 93847\n",
      "Edges in E_new 27044\n"
     ]
    }
   ],
   "source": [
    "# convert into nx Graphs\n",
    "g_train = nx.convert_matrix.from_pandas_edgelist(data_train,source = \"parent\",target=\"to\",\n",
    "                                                edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "g_test = nx.convert_matrix.from_pandas_edgelist(data_test,source = \"parent\",target=\"to\",\n",
    "                                                 edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "#find core\n",
    "core= find_core(g_train,g_test,3,3)\n",
    "e_new = g_test.number_of_edges()\n",
    "e_old = g_train.number_of_edges()\n",
    "print(\"Nodes in core {}\".format(core.number_of_nodes()))\n",
    "print(\"Edges in core {}\".format(core.number_of_edges()))\n",
    "print(\"Edges in E_old {}\\nEdges in E_new {}\".format(e_old,e_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "classifier = []\n",
    "dictionary = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>After finding the core graph i started the prediction phase.<br>\n",
    "Each predictor returns a score between two nodes u,v that rapresents how likely an edge (u,v) may form in the future.<br>\n",
    "I selected the edges that return the highest score and then verified the accuracy of the prediction.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Random predictor: 0.10%\n"
     ]
    }
   ],
   "source": [
    "rnd = linkpred.predictors.Random(g_train, excluded=g_train.edges())\n",
    "label = \"Random\"\n",
    "accuracy.append(predict_link(g_test,rnd,e_new,label))\n",
    "classifier.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Common Neighbours predictor: 3.01%\n"
     ]
    }
   ],
   "source": [
    "cn = linkpred.predictors.CommonNeighbours(g_train, excluded=g_train.edges())\n",
    "label = \"Common Neighbours\"\n",
    "accuracy.append(predict_link(g_test,cn,e_new,label))\n",
    "classifier.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Jaccard predictor: 2.54%\n"
     ]
    }
   ],
   "source": [
    "jc = linkpred.predictors.Jaccard(g_train, excluded=g_train.edges())\n",
    "label = \"Jaccard\"\n",
    "accuracy.append(predict_link(g_test,jc,e_new,label))\n",
    "classifier.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing matrix powers: [############################################################] 5/5\n",
      "Accuracy Katz, beta: 0.05 predictor: 3.26%\n",
      "Computing matrix powers: [############################################################] 5/5\n",
      "Accuracy Katz, beta: 0.005 predictor: 4.05%\n",
      "Computing matrix powers: [############################################################] 5/5\n",
      "Accuracy Katz, beta: 0.0005 predictor: 4.30%\n"
     ]
    }
   ],
   "source": [
    "betas = [0.05,0.005,0.0005]\n",
    "for beta in betas:\n",
    "    kz = linkpred.predictors.Katz(g_train, excluded=g_train.edges())\n",
    "    label = \"Katz, beta: {}\".format(beta)\n",
    "    accuracy.append(predict_link(g_test,kz,e_new,label,beta=beta))\n",
    "    classifier.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Graph Distance predictor: 1.12%\n"
     ]
    }
   ],
   "source": [
    "gd = linkpred.predictors.GraphDistance(g_train, excluded=g_train.edges())\n",
    "label = \"Graph Distance\"\n",
    "accuracy.append(predict_link(g_test,gd,e_new,label,alpha=0,weight=None))\n",
    "classifier.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[############################################################] 3364/3364\n",
      "Accuracy Rooted page rank, alpha: 0.01 predictor: 2.78%\n",
      "[############################################################] 3364/3364\n",
      "Accuracy Rooted page rank, alpha: 0.05 predictor: 2.80%\n",
      "[############################################################] 3364/3364\n",
      "Accuracy Rooted page rank, alpha: 0.15 predictor: 2.80%\n",
      "[############################################################] 3364/3364\n",
      "Accuracy Rooted page rank, alpha: 0.3 predictor: 2.75%\n",
      "[############################################################] 3364/3364\n",
      "Accuracy Rooted page rank, alpha: 0.5 predictor: 2.75%\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.01,0.05,0.15,0.30,0.50]\n",
    "for alpha in alphas:\n",
    "    rpg = linkpred.predictors.RootedPageRank(g_train, excluded=g_train.edges())\n",
    "    label = \"Rooted page rank, alpha: {}\".format(alpha)\n",
    "    accuracy.append(predict_link(g_test,rpg,e_new,label,alpha=alpha,weight=None))\n",
    "    classifier.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Simrank predictor: 0.26%\n"
     ]
    }
   ],
   "source": [
    "simrank = linkpred.predictors.SimRank(g_train, excluded=g_train.edges())\n",
    "simrank_results = simrank.predict(weight=None)\n",
    "label = \"Simrank\"\n",
    "accuracy.append(predict_link(g_test,simrank,e_new,label,weight=None))\n",
    "classifier.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Classifier  Accuracy\n",
      "0                          Random  0.000998\n",
      "1               Common Neighbours  0.030099\n",
      "2                         Jaccard  0.025366\n",
      "3                Katz, beta: 0.05  0.032577\n",
      "4               Katz, beta: 0.005  0.040527\n",
      "5              Katz, beta: 0.0005  0.042967\n",
      "6                  Graph Distance  0.011167\n",
      "7   Rooted page rank, alpha: 0.01  0.027770\n",
      "8   Rooted page rank, alpha: 0.05  0.027954\n",
      "9   Rooted page rank, alpha: 0.15  0.027954\n",
      "10   Rooted page rank, alpha: 0.3  0.027511\n",
      "11   Rooted page rank, alpha: 0.5  0.027548\n",
      "12                        Simrank  0.002625\n"
     ]
    }
   ],
   "source": [
    "dictionary[\"Classifier\"] = classifier\n",
    "dictionary[\"Accuracy\"] = accuracy\n",
    "clf_results = pd.DataFrame(dictionary)\n",
    "clf_results.to_csv(\"clf_results.csv\")\n",
    "print (clf_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Even tough we achieved on avarage a performane 10 times better than a random predictor, overall the results are not good. \n",
    "At most we achieved an accuracy of 4.30%.\n",
    "This is due to different reasons:<br>\n",
    "1. The internet is imprevedible: <br>\n",
    "New trends,memes and topic of discussions may arise at any moment without notice.<br>\n",
    "2. We used posts in the training set that are too old:<br>\n",
    "The majority of the crossposts are fairily new, infact the test set is composed of posts at most two weeks old while the  training set contains posts from 2017. Since the internet changes so quickly, relationship between subreddits that are this old, may lead to wrong results.<br>\n",
    "\n",
    "To test this hypotesis I repeated the precedent steps considering only the posts posted from march 2021 onwards</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique posts 62387\n",
      "Edges in trainig set 68947\n",
      "Edges in test set 28366\n",
      "7853 nodes removed\n",
      "5353 nodes removed\n",
      "Nodes in core 2574\n",
      "Edges in core 50142\n",
      "Edges in E_old 40528\n",
      "Edges in E_new 17946\n"
     ]
    }
   ],
   "source": [
    "    #set the date that separates test and training set\n",
    "    START_DATE = 1618631200\n",
    "    # load and clean data\n",
    "    data = pd.read_csv(\"../scraping data/data/data_subreddit_cleaned.csv\",index_col=0)\n",
    "    data = remove_useless_data(data)\n",
    "    data = data[data[\"date\"] > 1614556800]\n",
    "    #divide in training and test set\n",
    "    data_train = data[data[\"date\"] < START_DATE]\n",
    "    data_test = data[data[\"date\"] >= START_DATE]\n",
    "    print(\"Number of unique posts {}\".format(len(pd.unique(data[\"title\"]))))\n",
    "    print(\"Edges in trainig set {}\\nEdges in test set {}\".format(\n",
    "                    data_train.shape[0],data_test.shape[0]))\n",
    "\n",
    "    # convert into nx Graphs\n",
    "    g_train = nx.convert_matrix.from_pandas_edgelist(data_train,source = \"parent\",target=\"to\",\n",
    "                                                     edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "    g_test = nx.convert_matrix.from_pandas_edgelist(data_test,source = \"parent\",target=\"to\",\n",
    "\n",
    "                                                     edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "    #find core\n",
    "    core= find_core(g_train,g_test,3,3)\n",
    "    e_new = g_test.number_of_edges()\n",
    "    e_old = g_train.number_of_edges()\n",
    "    print(\"Nodes in core {}\".format(core.number_of_nodes()))\n",
    "    print(\"Edges in core {}\".format(core.number_of_edges()))\n",
    "    print(\"Edges in E_old {}\\nEdges in E_new {}\".format(e_old,e_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Common Neighbours predictor: 4.25%\n"
     ]
    }
   ],
   "source": [
    "cn = linkpred.predictors.CommonNeighbours(g_train, excluded=g_train.edges())\n",
    "label = \"Common Neighbours\"\n",
    "accuracy.append(predict_link(g_test,cn,e_new,label))\n",
    "classifier.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Jaccard predictor: 3.62%\n"
     ]
    }
   ],
   "source": [
    "jc = linkpred.predictors.Jaccard(g_train, excluded=g_train.edges())\n",
    "label = \"Jaccard\"\n",
    "accuracy.append(predict_link(g_test,jc,e_new,label))\n",
    "classifier.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing matrix powers: [############################################################] 5/5\n",
      "Accuracy Katz, beta: 0.05 predictor: 4.23%\n",
      "Computing matrix powers: [############################################################] 5/5\n",
      "Accuracy Katz, beta: 0.005 predictor: 5.09%\n",
      "Computing matrix powers: [############################################################] 5/5\n",
      "Accuracy Katz, beta: 0.0005 predictor: 5.08%\n"
     ]
    }
   ],
   "source": [
    "betas = [0.05,0.005,0.0005]\n",
    "for beta in betas:\n",
    "    kz = linkpred.predictors.Katz(g_train, excluded=g_train.edges())\n",
    "    label = \"Katz, beta: {}\".format(beta)\n",
    "    accuracy.append(predict_link(g_test,kz,e_new,label,beta=beta))\n",
    "    classifier.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>This way I obtained better results but the overall accuracy is still too low.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
